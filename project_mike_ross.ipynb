{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MD4K9G6o6LE"
      },
      "source": [
        "### **Installing libraries that is requirred for the project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tewrjzreonT1"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_tlmgkeuUt0"
      },
      "source": [
        "## **Importing necessary modules or libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zW4feWM7o7em"
      },
      "outputs": [],
      "source": [
        "# import necessary modules.\n",
        "import base64\n",
        "import copy\n",
        "import json\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "try:\n",
        "\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sjIYvpqvbrQ"
      },
      "source": [
        "# **Loading the model and pass the arguements of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gtnZkq2Uvp92"
      },
      "outputs": [],
      "source": [
        "# Parse the arguments\n",
        "\n",
        "model = 'gemini-1.5-flash' # @param {isTemplate: true}\n",
        "contents_b64 = 'W3sicm9sZSI6InVzZXIiLCJwYXJ0cyI6W3sidGV4dCI6ImVzc2F5IG9uIGFpIn1dfSx7InJvbGUiOiJtb2RlbCIsInBhcnRzIjpbeyJ0ZXh0IjoiIyMgVGhlIEV2b2x2aW5nIExhbmRzY2FwZSBvZiBBcnRpZmljaWFsIEludGVsbGlnZW5jZTogQSBTeW1waG9ueSBvZiBQcm9ncmVzcyBhbmQgVW5jZXJ0YWludHlcXG5cXG5BcnRpZmljaWFsIGludGVsbGlnZW5jZSAoQUkpIGhhcyB0cmFuc2l0aW9uZWQgZnJvbSBhIHNjaWVuY2UgZmljdGlvbiBjb25jZXB0IHRvIGEgcGVydmFzaXZlIGZvcmNlIGluIG91ciBtb2Rlcm4gd29ybGQuICBJdCBpcyByZXNoYXBpbmcgaW5kdXN0cmllcywgYXV0b21hdGluZyB0YXNrcywgYW5kIGV2ZW4gaW5mbHVlbmNpbmcgb3VyIHBlcnNvbmFsIGludGVyYWN0aW9ucy4gIFRoaXMgZXNzYXkgd2lsbCBkZWx2ZSBpbnRvIHRoZSBtdWx0aWZhY2V0ZWQgbmF0dXJlIG9mIEFJLCBleHBsb3JpbmcgaXRzIHBvdGVudGlhbCBiZW5lZml0cywgZXRoaWNhbCBjb25zaWRlcmF0aW9ucywgYW5kIHRoZSBwcm9mb3VuZCBpbXBhY3QgaXQgaGFzIG9uIGh1bWFuaXR5J3MgZnV0dXJlLlxcblxcbkF0IHRoZSBjb3JlIG9mIEFJIGxpZXMgdGhlIGFzcGlyYXRpb24gdG8gbWltaWMgaHVtYW4gaW50ZWxsaWdlbmNlLiAgVGhyb3VnaCBzb3BoaXN0aWNhdGVkIGFsZ29yaXRobXMgYW5kIG1hY2hpbmUgbGVhcm5pbmcsIEFJIHN5c3RlbXMgYXJlIHRyYWluZWQgdG8gYW5hbHl6ZSB2YXN0IGFtb3VudHMgb2YgZGF0YSwgcmVjb2duaXplIHBhdHRlcm5zLCBhbmQgbWFrZSBwcmVkaWN0aW9ucy4gIFRoaXMgYWJpbGl0eSBoYXMgbGVkIHRvIHJlbWFya2FibGUgYWR2YW5jZW1lbnRzIGluIHZhcmlvdXMgZmllbGRzLiBJbiBoZWFsdGhjYXJlLCBBSSBhc3Npc3RzIGluIGRpYWdub3NpbmcgZGlzZWFzZXMsIGRldmVsb3BpbmcgcGVyc29uYWxpemVkIHRyZWF0bWVudCBwbGFucywgYW5kIGV2ZW4gYWNjZWxlcmF0aW5nIGRydWcgZGlzY292ZXJ5LiAgSW4gZmluYW5jZSwgaXQgZW1wb3dlcnMgYWxnb3JpdGhtcyB0byBhbmFseXplIG1hcmtldCB0cmVuZHMsIG1hbmFnZSByaXNrLCBhbmQgb3B0aW1pemUgaW52ZXN0bWVudCBzdHJhdGVnaWVzLiAgVGhlIGxpc3Qgb2YgYXBwbGljYXRpb25zIGdvZXMgb24sIGZyb20gc2VsZi1kcml2aW5nIGNhcnMgdG8gcGVyc29uYWxpemVkIHZpcnR1YWwgYXNzaXN0YW50cywgZGVtb25zdHJhdGluZyB0aGUgd2lkZS1yZWFjaGluZyBpbXBhY3Qgb2YgQUkuXFxuXFxuSG93ZXZlciwgdGhlIHJpc2Ugb2YgQUkgYWxzbyBicmluZ3Mgd2l0aCBpdCBhIGNvbXBsZXggd2ViIG9mIGV0aGljYWwgY29uY2VybnMuICBPbmUgY3J1Y2lhbCBhcmVhIGlzIGJpYXMuICBBSSBzeXN0ZW1zLCB0cmFpbmVkIG9uIGhpc3RvcmljYWwgZGF0YSwgbWF5IGluYWR2ZXJ0ZW50bHkgaW5oZXJpdCBhbmQgYW1wbGlmeSBleGlzdGluZyBiaWFzZXMsIGxlYWRpbmcgdG8gdW5mYWlyIG9yIGRpc2NyaW1pbmF0b3J5IG91dGNvbWVzLiAgVGhpcyByYWlzZXMgY3JpdGljYWwgcXVlc3Rpb25zIGFib3V0IGFjY291bnRhYmlsaXR5LCB0cmFuc3BhcmVuY3ksIGFuZCB0aGUgbmVlZCBmb3Igcm9idXN0IGV0aGljYWwgZnJhbWV3b3JrcyB0byBlbnN1cmUgZmFpcm5lc3MgYW5kIGVxdWl0YWJsZSBhcHBsaWNhdGlvbiBvZiBBSS5cXG5cXG5Bbm90aGVyIHNpZ25pZmljYW50IGNvbmNlcm4gaXMgdGhlIHBvdGVudGlhbCBmb3Igam9iIGRpc3BsYWNlbWVudC4gIEFzIEFJIGF1dG9tYXRlcyB0YXNrcyBwcmV2aW91c2x5IHBlcmZvcm1lZCBieSBodW1hbnMsIGNvbmNlcm5zIGFib3V0IHVuZW1wbG95bWVudCBhbmQgc29jaWV0YWwgZGlzcnVwdGlvbiBhcmlzZS4gIFRoaXMgZGVtYW5kcyBhIHByb2FjdGl2ZSBhcHByb2FjaCB0byByZXNraWxsaW5nIGFuZCB1cHNraWxsaW5nIHRoZSB3b3JrZm9yY2UsIGVuc3VyaW5nIHRoYXQgaW5kaXZpZHVhbHMgY2FuIGFkYXB0IHRvIHRoZSBldm9sdmluZyBqb2IgbWFya2V0IGFuZCBlbWJyYWNlIG9wcG9ydHVuaXRpZXMgY3JlYXRlZCBieSBBSS5cXG5cXG5GdXJ0aGVybW9yZSwgdGhlIHJhcGlkIGFkdmFuY2VtZW50IG9mIEFJIHJhaXNlcyBxdWVzdGlvbnMgYWJvdXQgaXRzIHBvdGVudGlhbCBpbmZsdWVuY2Ugb24gaHVtYW4gYXV0b25vbXkuICBUaGUgcmVsaWFuY2Ugb24gQUkgZm9yIGRlY2lzaW9uLW1ha2luZywgcGFydGljdWxhcmx5IGluIGNyaXRpY2FsIGFyZWFzIGxpa2UgaGVhbHRoY2FyZSBhbmQgbGF3IGVuZm9yY2VtZW50LCByYWlzZXMgY29uY2VybnMgYWJvdXQgdGhlIGVyb3Npb24gb2YgaHVtYW4gYWdlbmN5IGFuZCB0aGUgbmVlZCB0byBtYWludGFpbiBjb250cm9sIG92ZXIgb3VyIG93biBkZXN0aW5pZXMuICBUaGlzIGNhbGxzIGZvciBvbmdvaW5nIGRpYWxvZ3VlIGFuZCBkZWxpYmVyYXRpb24gcmVnYXJkaW5nIHRoZSBib3VuZGFyaWVzIG9mIEFJIGFuZCBpdHMgaW50ZWdyYXRpb24gaW50byBodW1hbiBsaWZlLlxcblxcblRoZSBmdXR1cmUgb2YgQUkgcmVtYWlucyBhIHRhcGVzdHJ5IHdvdmVuIHdpdGggYm90aCBwcm9taXNlIGFuZCB1bmNlcnRhaW50eS4gIFdoaWxlIGl0cyBwb3RlbnRpYWwgdG8gZW5oYW5jZSBvdXIgbGl2ZXMsIHNvbHZlIGNvbXBsZXggcHJvYmxlbXMsIGFuZCBwdXNoIHRoZSBib3VuZGFyaWVzIG9mIGh1bWFuIGtub3dsZWRnZSBpcyBpbW1lbnNlLCBpdCBpcyBpbXBlcmF0aXZlIHRvIGFkZHJlc3MgaXRzIGV0aGljYWwgYW5kIHNvY2lldGFsIGltcGxpY2F0aW9ucy4gIEJ5IGZvc3RlcmluZyBvcGVuIGRpYWxvZ3VlLCBwcm9tb3RpbmcgcmVzcG9uc2libGUgZGV2ZWxvcG1lbnQsIGFuZCBlc3RhYmxpc2hpbmcgZXRoaWNhbCBndWlkZWxpbmVzLCB3ZSBjYW4gaGFybmVzcyB0aGUgdHJhbnNmb3JtYXRpdmUgcG93ZXIgb2YgQUkgd2hpbGUgc2FmZWd1YXJkaW5nIG91ciB2YWx1ZXMgYW5kIGVuc3VyaW5nIGEgZnV0dXJlIHdoZXJlIHRlY2hub2xvZ3kgc2VydmVzIGh1bWFuaXR5LiBcXG5cXG5VbHRpbWF0ZWx5LCBBSSBpcyBub3QgYSB0aHJlYXQsIGJ1dCByYXRoZXIgYSBwb3dlcmZ1bCB0b29sLiAgSXQgaXMgb3VyIHJlc3BvbnNpYmlsaXR5IHRvIHdpZWxkIGl0IHdpdGggd2lzZG9tLCBjb21wYXNzaW9uLCBhbmQgYSBkZWVwIHVuZGVyc3RhbmRpbmcgb2YgaXRzIGltcGFjdCBvbiBvdXIgc29jaWV0eSwgc2hhcGluZyBhIGZ1dHVyZSB3aGVyZSBBSSBlbXBvd2VycyB1cyB0byByZWFjaCBvdXIgZnVsbCBwb3RlbnRpYWwgYXMgaHVtYW4gYmVpbmdzLiBcXG4ifV19XQ==' # @param {isTemplate: true}\n",
        "generation_config_b64 = 'eyJ0ZW1wZXJhdHVyZSI6MSwidG9wX3AiOjAuOTUsInRvcF9rIjo0MCwibWF4X291dHB1dF90b2tlbnMiOjgxOTJ9' # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceAJs_sXwGSi"
      },
      "source": [
        "# **Converting input prompt into files and pass them into the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tw8hFAZpTU-",
        "outputId": "83120cb3-ca9c-49cd-c612-e991bf71e059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"essay on ai\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"model\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"## The Evolving Landscape of Artificial Intelligence: A Symphony of Progress and Uncertainty\\\\n\\\\nArtificial intelligence (AI) has transitioned from a science fiction concept to a pervasive force in our modern world.  It is reshaping industries, automating tasks, and even influencing our personal interactions.  This essay will delve into the multifaceted nature of AI, exploring its potential benefits, ethical considerations, and the profound impact it has on humanity's future.\\\\n\\\\nAt the core of AI lies the aspiration to mimic human intelligence.  Through sophisticated algorithms and machine learning, AI systems are trained to analyze vast amounts of data, recognize patterns, and make predictions.  This ability has led to remarkable advancements in various fields. In healthcare, AI assists in diagnosing diseases, developing personalized treatment plans, and even accelerating drug discovery.  In finance, it empowers algorithms to analyze market trends, manage risk, and optimize investment strategies.  The list of applications goes on, from self-driving cars to personalized virtual assistants, demonstrating the wide-reaching impact of AI.\\\\n\\\\nHowever, the rise of AI also brings with it a complex web of ethical concerns.  One crucial area is bias.  AI systems, trained on historical data, may inadvertently inherit and amplify existing biases, leading to unfair or discriminatory outcomes.  This raises critical questions about accountability, transparency, and the need for robust ethical frameworks to ensure fairness and equitable application of AI.\\\\n\\\\nAnother significant concern is the potential for job displacement.  As AI automates tasks previously performed by humans, concerns about unemployment and societal disruption arise.  This demands a proactive approach to reskilling and upskilling the workforce, ensuring that individuals can adapt to the evolving job market and embrace opportunities created by AI.\\\\n\\\\nFurthermore, the rapid advancement of AI raises questions about its potential influence on human autonomy.  The reliance on AI for decision-making, particularly in critical areas like healthcare and law enforcement, raises concerns about the erosion of human agency and the need to maintain control over our own destinies.  This calls for ongoing dialogue and deliberation regarding the boundaries of AI and its integration into human life.\\\\n\\\\nThe future of AI remains a tapestry woven with both promise and uncertainty.  While its potential to enhance our lives, solve complex problems, and push the boundaries of human knowledge is immense, it is imperative to address its ethical and societal implications.  By fostering open dialogue, promoting responsible development, and establishing ethical guidelines, we can harness the transformative power of AI while safeguarding our values and ensuring a future where technology serves humanity. \\\\n\\\\nUltimately, AI is not a threat, but rather a powerful tool.  It is our responsibility to wield it with wisdom, compassion, and a deep understanding of its impact on our society, shaping a future where AI empowers us to reach our full potential as human beings. \\\\n\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Convert and upload the files\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data, index):\n",
        "    \"\"\"Upload files to the Files API.\n",
        "\n",
        "    For each file, Google AI Studio either sent:\n",
        "    - a Google Drive ID,\n",
        "    - a URL,\n",
        "    - a file path, or\n",
        "    - The raw bytes (`inline_data`).\n",
        "\n",
        "    The API only understands `inline_data` or it's Files API.\n",
        "    This code, uploads files to the files API where the API can access them.\n",
        "    \"\"\"\n",
        "\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        path = tempfiles / str(index)\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"📁 Files\" in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "\n",
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "index = 0\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data, index)\n",
        "            index += 1\n",
        "\n",
        "import json\n",
        "print(json.dumps(contents, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAKi_5U7xENy"
      },
      "source": [
        "# **Defining a function to call the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2RhWwVSnpmTf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Call the model and print the response.\n",
        "def prompt(model,prompt):\n",
        "  gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "  response = gemini.generate_content(\"provide suitable legal action\"+prompt,\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings,\n",
        "        stream=stream,\n",
        "  )\n",
        "  display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsht3MOTyjnf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIVMwF6ImsVQ",
        "outputId": "66c57bb2-4fb9-4768-d6e0-f3371278a8ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9kvVRDgm3_P",
        "outputId": "06925fcb-d854-4dc4-8ef8-417a1d4ccd03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.83.234.246\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-rombIwm8lL",
        "outputId": "284d2b25-5b97-430a-952a-1c01fafc7337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.83.234.246:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://bitter-walls-trade.loca.lt\n",
            "/root/.npm/_npx/75ac80b86e83d4a2/node_modules/localtunnel/bin/lt.js:81\n",
            "    throw err;\n",
            "    ^\n",
            "\n",
            "Error: connection refused: localtunnel.me:22459 (check your firewall settings)\n",
            "    at Socket.<anonymous> (/root/.npm/_npx/75ac80b86e83d4a2/node_modules/\u001b[4mlocaltunnel\u001b[24m/lib/TunnelCluster.js:52:11)\n",
            "\u001b[90m    at Socket.emit (node:events:513:28)\u001b[39m\n",
            "\u001b[90m    at emitErrorNT (node:internal/streams/destroy:157:8)\u001b[39m\n",
            "\u001b[90m    at emitErrorCloseNT (node:internal/streams/destroy:122:3)\u001b[39m\n",
            "\u001b[90m    at processTicksAndRejections (node:internal/process/task_queues:83:21)\u001b[39m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0vh6f20nATq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}